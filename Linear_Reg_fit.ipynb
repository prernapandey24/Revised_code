{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sbn\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from itertools import permutations, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36cc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train1 = 'data_base_demand_train_1_L_0.01.csv' #upload data with noleak case\n",
    "path_train2 = 'data_base_demand_train_2_L_0.01.csv'  #upload data with noleak case\n",
    "path_train3 = 'data_base_demand_train_3_L_0.01.csv'  #upload data with noleak case\n",
    "path_test_base_demand = 'data_base_demand_test_L_0.01.csv' #upload data with noleak case\n",
    "\n",
    "path_leak_train  = 'leak_train_L_0.01.csv' #upload data with leak case\n",
    "path_leak_test = 'leak_test_L_0.01.csv'    #upload data with leak case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy for historical observations\n",
    "df_train_reg_model = pd.read_csv(path_train1)\n",
    "df_train_class_model_ref = pd.read_csv(path_train2)\n",
    "df_train_class_model_rec = pd.read_csv(path_train3)\n",
    "\n",
    "# Proxy for recent observations, those need to be tested for leak\n",
    "df_test_class_model = pd.read_csv(path_test_base_demand)\n",
    "\n",
    "leak_training = pd.read_csv(path_leak_train)\n",
    "leak_testing = pd.read_csv(path_leak_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(df,link_names,head_names):\n",
    "    \n",
    "    data_flow = np.array(df[link_names])  # convering to litres per sec \n",
    "    data_head = np.array(df[head_names])\n",
    "    \n",
    "\n",
    "    train_out= data_head[:,0] - data_head[:,1] # deltaH\n",
    "    train_in = data_flow                       # flow1, flow2\n",
    "    \n",
    "    return train_in, train_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775075fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define a sensor list\n",
    "\n",
    "sensor_list = [[1,1],[2,2],[8,8]]\n",
    "\n",
    "sen_nums = np.arange(1,len(sensor_list)+1)\n",
    "combs = list(combinations(sen_nums,2))\n",
    "print(sen_nums)\n",
    "combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916698c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chk how combs can help in iterating through all possible combinations of sensors\n",
    "i=2\n",
    "print(len(combs))\n",
    "print(combs[i])\n",
    "print('first sensor is ',combs[i][0])\n",
    "print('second sensor is ',combs[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38dbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract input (flow1, flow2) and output (deltaH) data from datasets prepared earlier\n",
    "# data without leaks\n",
    "\n",
    "def data_in_out_noleak(sensor_pair,df):\n",
    "    \n",
    "    h1= sensor_list[sensor_pair[0]-1][0]\n",
    "    h2= sensor_list[sensor_pair[1]-1][0]\n",
    "    f1= sensor_list[sensor_pair[0]-1][1]\n",
    "    f2= sensor_list[sensor_pair[1]-1][1]\n",
    "    \n",
    "    link_name = ['Link_flow'+str(f1),'Link_flow'+str(f2)] \n",
    "    node_name = ['Node_head'+str(h1),'Node_head'+str(h2)]\n",
    "    \n",
    "    data_in, data_out = training_data(df,link_name,node_name)\n",
    "    \n",
    "    return data_in, data_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30215ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract input and output data with leak\n",
    "\n",
    "def data_in_out_withleak(sensor_pair,df):\n",
    "    \n",
    "    h1= sensor_list[sensor_pair[0]-1][0]\n",
    "    h2= sensor_list[sensor_pair[1]-1][0]\n",
    "    f1= sensor_list[sensor_pair[0]-1][1]\n",
    "    f2= sensor_list[sensor_pair[1]-1][1]\n",
    "    \n",
    "    link_name_leak = ['Link_flow'+str(f1),'Link_flow'+str(f2)]\n",
    "    node_name_leak = ['Node_head'+str(h1),'Node_head'+str(h2)]    \n",
    "    data_in, data_out = training_data(df,link_name_leak,node_name_leak)\n",
    "    \n",
    "    return data_in, data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comb in combs:    \n",
    "    xtrain, ytrain = data_in_out_noleak(comb,df_train_reg_model)    \n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    x_train_poly = poly.fit_transform(xtrain)    \n",
    "    lin_model = LinearRegression()\n",
    "    lin_model.fit(x_train_poly,ytrain)\n",
    "    \n",
    "    pkl_filename = 'linmodel'+str(comb[0])+str(comb[1])+'.pkl'\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(lin_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ab13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate column names\n",
    "colnames = []\n",
    "for comb in combs:\n",
    "    n1='obs'+str(comb[0])+str(comb[1])\n",
    "    n2='prd'+str(comb[0])+str(comb[1])\n",
    "    n3='stat'+str(comb[0])+str(comb[1])\n",
    "    n4='pval'+str(comb[0])+str(comb[1])\n",
    "    colnames.extend([n1,n2,n3,n4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datafortrees(numcases,df_recent,sample_len,casetype):\n",
    "    \n",
    "    df_cases = pd.DataFrame(columns=colnames)\n",
    "    #fracsize_recent=sample_len/len(df_recent)\n",
    "    fracsize_recent=0.3\n",
    "    print(fracsize_recent)\n",
    "    \n",
    "    fracsize_reference = sample_len/len(df_train_class_model_ref)\n",
    "    \n",
    "    # first loop to randomly select a sample test set\n",
    "\n",
    "    for i in range(numcases):\n",
    "        df_recent_sample = df_recent.sample(frac=fracsize_recent)\n",
    "        df_reference_sample = df_train_class_model_ref.sample(frac=fracsize_reference)\n",
    "        \n",
    "        # second loop to cover all possible leak combinations\n",
    "\n",
    "        comb_data = []\n",
    "        for comb in combs:\n",
    "            if casetype=='noleak':\n",
    "                xtest_rec, ytest_rec = data_in_out_noleak(comb,df_recent_sample)\n",
    "            else:\n",
    "                xtest_rec, ytest_rec = data_in_out_withleak(comb,df_recent_sample)\n",
    "                \n",
    "            xtest_ref, ytest_ref = data_in_out_noleak(comb,df_reference_sample)\n",
    "            \n",
    "\n",
    "            # load the linear regression model, make predictions and store results\n",
    "            pkl_filename = 'linmodel'+str(comb[0])+str(comb[1])+'.pkl'\n",
    "\n",
    "            with open(pkl_filename, 'rb') as file:\n",
    "                lin_model = pickle.load(file)\n",
    "\n",
    "            xtest_ref_poly = poly.fit_transform(xtest_ref)\n",
    "            xtest_rec_poly = poly.fit_transform(xtest_rec) \n",
    "            pred_ref = lin_model.predict(xtest_ref_poly).reshape(-1)\n",
    "            pred_rec = lin_model.predict(xtest_rec_poly).reshape(-1)\n",
    "            error_ref = (ytest_ref-pred_ref)\n",
    "            error_rec = (ytest_rec-pred_rec)\n",
    "            stat,pval = ks_2samp(error_ref,error_rec) #to test the model runs fine\n",
    "            comb_list = [np.mean(error_ref),np.mean(error_rec),stat,pval]            \n",
    "            comb_data.extend(comb_list)\n",
    "            \n",
    "        comb_series = pd.Series(comb_data,index=df_cases.columns)\n",
    "        #print(type(comb_series))\n",
    "        comb_series_1 = pd.DataFrame(comb_series)\n",
    "        comb_series_2 = comb_series_1.T\n",
    "        \n",
    "        \n",
    "        df_cases=pd.concat([df_cases, comb_series_2])\n",
    "        #print(type(df_cases))\n",
    "       \n",
    "        \n",
    "      \n",
    "    return df_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_training.leak_link.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining size of training set. Prefer choosing a multiple of 6 \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "num_samples = 100\n",
    "num_training_samples_each = 20000\n",
    "# num_training_sample_total = 3000\n",
    "# num_training_sample_leak = int(num_training_sample_total/6)\n",
    "# num_training_sample_noleak = num_training_sample_total - 3* (num_training_sample_leak)\n",
    "#warning('off')\n",
    "class_train_noleak = datafortrees(num_training_samples_each,df_train_class_model_rec,num_samples,'noleak')\n",
    "\n",
    "class_train_noleak['leak']=0\n",
    "class_train_noleak['leak_num']=0\n",
    "\n",
    "class_train_all_leaks = pd.DataFrame(columns = class_train_noleak.columns)\n",
    "# loop through all the links with leakage and prepare the dataframe\n",
    "for i in leak_training.leak_link.unique():\n",
    "    print(i)\n",
    "    leak_df = leak_training[leak_training.leak_link == i].reset_index(drop=True)\n",
    "    print(len(leak_df))\n",
    "    class_train = datafortrees(num_training_samples_each,leak_df,num_samples,'leak')\n",
    "    class_train['leak']=1\n",
    "    class_train['leak_num']=i\n",
    "    #class_train_all_leaks = class_train_all_leaks.append(class_train)\n",
    "    class_test_all_leaks=pd.concat([class_train_all_leaks,class_train], ignore_index=True)\n",
    "\n",
    "data_train_classification = pd.concat([class_train_noleak,class_train_all_leaks],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64413e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples= 100\n",
    "num_test_samples_each = 500\n",
    "# num_test_sample_leak = int(num_test_sample_total/6)\n",
    "# num_test_sample_noleak = num_test_sample_total - 3* (num_test_sample_leak)\n",
    "\n",
    "class_test_noleak = datafortrees(num_test_samples_each,df_test_class_model,num_samples,'noleak')\n",
    "class_test_noleak['leak']=0\n",
    "class_test_noleak['leak_num']=0\n",
    "\n",
    "class_test_all_leaks = pd.DataFrame(columns = class_test_noleak.columns)\n",
    "# loop through all the links with leakage and prepare the dataframe\n",
    "for i in leak_testing.leak_link.unique():\n",
    "    leak_df = leak_testing[leak_testing.leak_link == i].reset_index(drop=True)\n",
    "    class_test = datafortrees(num_test_samples_each,leak_df,num_samples,'leak')\n",
    "    #print((class_test))\n",
    "    class_test['leak']=1\n",
    "    class_test['leak_num']=i\n",
    "    #class_test_all_leaks = class_test_all_leaks.join(class_test)\n",
    "    #class_test_all_leaks=pd.concat([class_test_all_leaks,class_test],ignore_index=True)\n",
    "    class_test_all_leaks=pd.concat([class_test_all_leaks,class_test], ignore_index=True)\n",
    "    #class_test_all_leaks=pd.merge(class_test_all_leaks,class_test, left_index=True, right_index=True)\n",
    "    \n",
    "    \n",
    "\n",
    "data_test_classification = pd.concat([class_test_noleak,class_test_all_leaks],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmat_pd.to_csv('pp1.csv')\n",
    "data_test_classification.to_csv('data_test_classification_cor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966032c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_classification.to_csv('data_train_classification_cor.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
